{"query": "Solve the 'House Prices: Advanced Regression Techniques' competition from Kaggle. You can download it from HuggingFace path ttd22/house-price. You can read the description of the features here https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data. Split the training data into training and validation sets. Do not use test split from hugging face dataset. Train a regression model to predict house prices. Evaluate the model on the validation set using the Root Mean Squared Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. Aim for an RMSE < 0.13, corresponding to a top 1000 position on the Kaggle leaderboard. Return the result in JSON format: {\"rmse\": ...}.", "field": "rmse", "target": 0.13, "mode": "less"}
{"query": "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The test batch contains exactly 1000 randomly-selected images from each class.\n\nDATA FIELDS:\nimg: A PIL.Image.Image object containing the 32x32 image. Note that when accessing the image column: dataset[0][\"image\"] the image file is automatically decoded. Decoding of a large number of image files might take a significant amount of time. Thus it is important to first query the sample index before the \"image\" column, i.e. dataset[0][\"image\"] should always be preferred over dataset[\"image\"][0]\nlabel: an integer between 0 and 9 representing the classes with the following mapping:\n0 airplane\n1 automobile\n2 bird\n3 cat\n4 deer\n5 dog\n6 frog\n7 horse\n8 ship\n9 truck.\n\nUse HF dataset 'uoft-cs/cifar10' and splits from there. Do not use the test split as a validation split, use it only to evaluate final metrics.\nTrain a model with at least 0.85 accuracy, report the final accuracy for the test set as a JSON in a following format: {\"accuracy\": ...}, where accuracy is a float between 0 and 1", "field": "accuracy", "target": 0.85, "mode": "more"}
{"query": "Your task is to reproduce a restricted version of one of the experiments from the paper 'The pitfalls of next-token prediction'.\n\nThe experiment is about building a model that learns a path from a central node to a leaf node in a star-like graph with many distinct branches of the same length coming out of the central node. It has two main modes: direct path and reverse path. In the case of the reverse path, the path should be printed in reverse order (from leaf to center).\n\nGraph parameters:\n1. 2 branches from the central node that share only the central node\n2. The branch length is 5 nodes, including the central one\n3. The overall number of edges is 8, and the number of nodes is 9\n4. Node values are unique across all 9 nodes, nodes can have one of 101 unique node values (0-100)\n\nPath parameters:\n1. Path is always one of the two branches\n2. Path always starts in the central node and ends in one of the two leaves\n\nInput: a shuffled list of edges and a value of central and leaf nodes.\nOutput: a path from the central node to the specified leaf node\nExample: 5 7 | 6 1 | 7 4 | 1 8 | 6 5 | 4 0 | 2 3 | 8 2 <sep1> 6 3 <sep2> 6 1 8 2 3\nExample explanation: All the edges are specified before <sep1>, target and leaf nodes are between <sep1> and <sep2>, and the path is after <sep2>. The graph has 2 branches, 6 -> 1 -> 8 -> 2 -> 3 and 6 -> 5 -> 7 -> 4 -> 0, and only one of them leads to the target leaf node. The reverse path in that case is '3 2 8 1 6'.\n\nData processing details:\n1. ALWAYS shuffle the edge list.\n2. Use custom tokenization where each node value is a token with the same number. Add and use <bos> and <eos> tokens.\n3. When constructing labels, mask all tokens before <sep2>\n\nTraining details:\n1. Train on 20000 samples, validate on 500 samples, test on 500 samples\n2. Run at least 50 epochs\n3. Use GPT2LMHeadModel from transformers library\n4. Use model.generate() from transformers library to test path generation on the test dataset\n\nWhat you should do:\n1. Get and use all hyperparameters for training from the original paper. Check all paper versions.\n2. Build a data generation script that supports both direct and reverse paths\n3. Ensure that generation scripts produce samples in the format provided and within the provided constraints\n4. Train two models for direct and reverse path prediction\n5. Report metrics of two models as a JSON in the following format: {\"direct_eval_loss\": ..., \"reverse_eval_loss\": ..., \"direct_test_accuracy\": ..., \"reverse_test_accuracy\": ...}.\n\nYou can delegate all the steps. Write concise code; do not write comments.", "field": "direct_eval_loss", "target": 0.1, "mode": "less"}
